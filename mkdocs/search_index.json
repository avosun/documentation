{
    "docs": [
        {
            "location": "/", 
            "text": "SERPS\n\n\nThe PHP Search Engine Result Page Spider\n\n\n\n\n\n\nSERPS is a scraping library for php. It considerably decrease the complexity required to analyse search engines.\n\n\n\n\nWeb scraping\n (web harvesting or web data extraction) is a computer software technique of extracting \ninformation from websites. Usually, such software programs simulate human exploration of the World Wide Web \nby either implementing low-level Hypertext Transfer Protocol (HTTP), \nor embedding a fully-fledged web browser, such as Mozilla Firefox.\n\n\nWikipedia\n\n\n\n\nWhat is it?\n\n\nSerps is a set of tools that ease the parsing of \npopular search engines\n (such as google, yahoo or bing).\nIt helps to parse \nSERP\n (Search Engine Result Page) and gives you a standard output of what is parsed.\n\n\nThe problem\n\n\nMost of times search engines don't want you to parse them, and they don't offer a documentation or a standard way \nto extract the results from the SERP and it's hard to write and maintain a scraper.\n\n\nThe solution\n\n\nTo solve this problems we \nanalysed\n how search engines behave and we built the necessary tools to\nwork with them, from the URL generation to the parsing of the results. \nAt the endpoint we offer a \nstandard and documented API\n.\n\n\nGetting Started\n\n\nLooking forward to work with the library? \n\n\n\n\nStart with the \noverview\n.\n\n\nBrowse the available \nsearch engines\n and \nhttp clients\n from the top menu.\n\n\n\n\nSupport \n issue \n\n\nYou have problems to get started with the library or you have general question? We'd like to hear about,\n\njoin us on gitter\n.\n\n\nYou spotted an issue with the library? Please report it on the \n\ngithub issue tracker\n.\n\n\nLicensing\n\n\nThe work is placed under the terms of the \nFair License\n.\n\n\n\n\nUsage of the works is permitted provided that this instrument is retained with the works, \nso that any entity that uses the works is notified of this instrument.\n\n\nDISCLAIMER: THE WORKS ARE WITHOUT WARRANTY.", 
            "title": "Home"
        }, 
        {
            "location": "/#serps", 
            "text": "The PHP Search Engine Result Page Spider    SERPS is a scraping library for php. It considerably decrease the complexity required to analyse search engines.   Web scraping  (web harvesting or web data extraction) is a computer software technique of extracting \ninformation from websites. Usually, such software programs simulate human exploration of the World Wide Web \nby either implementing low-level Hypertext Transfer Protocol (HTTP), \nor embedding a fully-fledged web browser, such as Mozilla Firefox.  Wikipedia", 
            "title": "SERPS"
        }, 
        {
            "location": "/#what-is-it", 
            "text": "Serps is a set of tools that ease the parsing of  popular search engines  (such as google, yahoo or bing).\nIt helps to parse  SERP  (Search Engine Result Page) and gives you a standard output of what is parsed.", 
            "title": "What is it?"
        }, 
        {
            "location": "/#the-problem", 
            "text": "Most of times search engines don't want you to parse them, and they don't offer a documentation or a standard way \nto extract the results from the SERP and it's hard to write and maintain a scraper.", 
            "title": "The problem"
        }, 
        {
            "location": "/#the-solution", 
            "text": "To solve this problems we  analysed  how search engines behave and we built the necessary tools to\nwork with them, from the URL generation to the parsing of the results. \nAt the endpoint we offer a  standard and documented API .", 
            "title": "The solution"
        }, 
        {
            "location": "/#getting-started", 
            "text": "Looking forward to work with the library?    Start with the  overview .  Browse the available  search engines  and  http clients  from the top menu.", 
            "title": "Getting Started"
        }, 
        {
            "location": "/#support-issue", 
            "text": "You have problems to get started with the library or you have general question? We'd like to hear about, join us on gitter .  You spotted an issue with the library? Please report it on the  github issue tracker .", 
            "title": "Support &amp; issue "
        }, 
        {
            "location": "/#licensing", 
            "text": "The work is placed under the terms of the  Fair License .   Usage of the works is permitted provided that this instrument is retained with the works, \nso that any entity that uses the works is notified of this instrument.  DISCLAIMER: THE WORKS ARE WITHOUT WARRANTY.", 
            "title": "Licensing"
        }, 
        {
            "location": "/overview/", 
            "text": "Overview\n\n\nThis overview will help you to understand how the library is built and what are its main components.\n\n\n\n\nInstall\n\n\nTo work with SERPS you need two things:\n\n\n\n\nOne or more search engine clients you want to parse\n\n\nA http client\n\n\n\n\nIn addition \nComposer\n is required to manage the necessary dependencies.\n\n\n\n\ncomposer.json example with the \nGoogle client\n and the \nCurl http client\n\n\n\n\n{\n    \nrequire\n: {\n        \nserps/core\n: \n*\n,\n        \nserps/search-engine-google\n: \n*\n,\n        \nserps/http-client-curl\n: \n*\n\n    }\n}\n\n\n\n\n\n\nDanger\n\n\nThe library is still in alpha, no version is released yet that means that minor things can change until \nthe stable release and your code might become not compatible with the updates.\n\n\n\n\nSearch Engine client\n\n\nIn a regular workflow a search engine client allows to:\n\n\n\n\nManipulate an url and generate a request specific to the search engine\n\n\nRetrieve the response from the search engine\n\n\nParse this response to a standard sets of results\n\n\n\n\nEach \nsearch engine\n has its set of specificities and thus each search engine implementation has its own dedicated guide.\n\n\nThese search engines are currently available:\n\n\n\n\nGoogle\n\n\n\n\nHttp Client\n\n\nWorking with search engines involves to work with \nhttp requests\n.\nUsually the \nsearch engine client\n will need a http client to work correctly.\n\n\n\n\nExample with the \ngoogle client\n and the \ncurl http client\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n\n\n\nThere are two kinds of http clients: those that return the raw html as returned from the search engine (e.g the curl client)\nand the others that evaluate the javascript and update the DOM before before returning (e.g the phantomJS client)\n\n\nThese http clients are currently available:\n\n\n\n\nRaw clients:\n\n\nCURL\n\n\n\n\n\n\nEvaluating clients\n\n\nphantomJS\n\n\n\n\n\n\n\n\nProxies\n\n\nMost of time search engines don't want you to parse them thus \nthey use to block you with captcha when they think you are a bot\nWhen you deal with a very \nlarge number of requests\n, you will need to send requests\nthrough proxies.\n\n\nThis is a major feature of scraping and we placed proxies at the very heart of the library. \nEach request is proxy aware. \nThis way, with a single client you can use as many proxies as you want.\n\n\n\n\nExample of \nproxy\n usage with the google client\n\n\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleClient-\nquery($googleUrl, $proxy);\n\n\n\n\nCaptcha\n\n\nEven though you are using proxies and place all the efforts to act like an human, you might encounter the fatal captcha.\n\n\nWhen you get \nblocked\n by a captcha request, it is very important to stop sending request to the search engine and to\nsolve the captcha before you continue. \n\n\nDealing with captcha is not easy, at the current state the library can detect captcha but is not able to solve them\nfor you we are \ncurrently working\n on a captcha solver implementation.\n\n\n\n\nNote\n\n\nCaptcha are proxy specific, when solving a captcha that should be done with the proxy that was initially blocked\n\n\n\n\nCookies\n\n\nSERPS integrates cookie management, that allows to share cookies across many requests.\n\n\nCookie management is usually done at the search engine client level. You still want to know\nhow to manipulate cookies and cookiejars: \n\nsee cookie documentation", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#overview", 
            "text": "This overview will help you to understand how the library is built and what are its main components.", 
            "title": "Overview"
        }, 
        {
            "location": "/overview/#install", 
            "text": "To work with SERPS you need two things:   One or more search engine clients you want to parse  A http client   In addition  Composer  is required to manage the necessary dependencies.   composer.json example with the  Google client  and the  Curl http client   {\n     require : {\n         serps/core :  * ,\n         serps/search-engine-google :  * ,\n         serps/http-client-curl :  * \n    }\n}   Danger  The library is still in alpha, no version is released yet that means that minor things can change until \nthe stable release and your code might become not compatible with the updates.", 
            "title": "Install"
        }, 
        {
            "location": "/overview/#search-engine-client", 
            "text": "In a regular workflow a search engine client allows to:   Manipulate an url and generate a request specific to the search engine  Retrieve the response from the search engine  Parse this response to a standard sets of results   Each  search engine  has its set of specificities and thus each search engine implementation has its own dedicated guide.  These search engines are currently available:   Google", 
            "title": "Search Engine client"
        }, 
        {
            "location": "/overview/#http-client", 
            "text": "Working with search engines involves to work with  http requests .\nUsually the  search engine client  will need a http client to work correctly.   Example with the  google client  and the  curl http client       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());  There are two kinds of http clients: those that return the raw html as returned from the search engine (e.g the curl client)\nand the others that evaluate the javascript and update the DOM before before returning (e.g the phantomJS client)  These http clients are currently available:   Raw clients:  CURL    Evaluating clients  phantomJS", 
            "title": "Http Client"
        }, 
        {
            "location": "/overview/#proxies", 
            "text": "Most of time search engines don't want you to parse them thus \nthey use to block you with captcha when they think you are a bot\nWhen you deal with a very  large number of requests , you will need to send requests\nthrough proxies.  This is a major feature of scraping and we placed proxies at the very heart of the library. \nEach request is proxy aware. \nThis way, with a single client you can use as many proxies as you want.   Example of  proxy  usage with the google client       use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleClient- query($googleUrl, $proxy);", 
            "title": "Proxies"
        }, 
        {
            "location": "/overview/#captcha", 
            "text": "Even though you are using proxies and place all the efforts to act like an human, you might encounter the fatal captcha.  When you get  blocked  by a captcha request, it is very important to stop sending request to the search engine and to\nsolve the captcha before you continue.   Dealing with captcha is not easy, at the current state the library can detect captcha but is not able to solve them\nfor you we are  currently working  on a captcha solver implementation.   Note  Captcha are proxy specific, when solving a captcha that should be done with the proxy that was initially blocked", 
            "title": "Captcha"
        }, 
        {
            "location": "/overview/#cookies", 
            "text": "SERPS integrates cookie management, that allows to share cookies across many requests.  Cookie management is usually done at the search engine client level. You still want to know\nhow to manipulate cookies and cookiejars:  see cookie documentation", 
            "title": "Cookies"
        }, 
        {
            "location": "/cookies/", 
            "text": "Cookies\n\n\nGuide for cookie and cookiejar manipulation \n\n\n\n\nSERPS offers convenient tools that emulate a cookiejar and allow to persist cookies across many requests.\n\n\nThe following examples introduce the work with cookies\n\n\n\n\nWarning\n\n\nCookies usage is still at prototype stage and all http engines do not support cookies yet.\n\n\n\n\nCreate a cookie\n\n\nParameters for creating a cookie:\n\n\n\n\nname\n: the name of a cookie.\n\n\nvalue\n: the value of a cookie.\n\n\nflags\n: cookie flags. Available flags are:\n\n\npath\n\n\ndomain\n\n\nexpires\n\n\ndiscard\n\n\nsecure\n\n\n\n\n\n\n\n\nuse Serps\\Core\\Cookie\\Cookie;\n\n$cookie = new Cookie('baz', 'bar', [\n    'domain' =\n 'foo.bar',\n    'path' =\n '/',\n    'expires' =\n time() + 1000,\n]);\n\n\n\n\nPopulate a Cookie Jar\n\n\nuse Serps\\Core\\Cookie\\ArrayCookieJar;\n\n$cookieJar = new ArrayCookieJar();\n\n// Add the cookie 'foo' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('foo', 'bar', ['domain' =\n 'foo.bar']);\n$cookieJar-\nset($cookie);\n\n// Add the cookie 'baz' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('baz', 'bar', ['domain' =\n 'foo.bar']);\n$cookieJar-\nset($cookie);\n\n\n\n\nRetrieves cookies\n\n\nThe \nall\n method is responsible for getting cookies matching some filters\n\n\nMethod parameters:\n\n\n\n\ndomain\n: filters the cookies matching the given domain. Pass null to match all domains.\n\n\npath\n: filters the cookies matching the given path. Pass null to match all paths.\n\n\nname\n: filters the cookies matching the given name. Pass null to match all names.\n\n\nskipDiscardable\n: Set to TRUE to skip cookies with the Discard attribute. Default FALSE.\n\n\nskipExpired\n: Set to FALSE to include expired. Default TRUE.\n\n\n\n\n// Retrieves all cookies\n$cookies = $cookieJar-\nall();\n\n// Retrieves all cookies matching the domain \nfoo.bar\n\n$cookies = $cookieJar-\nall(\nfoo.bar\n);\n\n\n// Retrieves the cookie named \nfoo\n for the domain \nfoo.bar\n, including expired cookies\n$cookies = $cookieJar-\nall(\nfoo.bar\n, \n/\n, \nfoo\n, false, false);\n\n\n\n\nRetrieve cookie for a request\n\n\nIt's possible to automatically retrieve cookies that match a given PSR-7 request:\n\n\n// retrieves all cookies matching the request\n$cookies = $cookieJar-\ngetMatchingCookies($request);\n\n\n\n\nRemove cookies\n\n\n// Remove all cookies\n$cookies = $cookieJar-\nremove();\n\n// Remove all cookies matching the domain \nfoo.bar\n\n$cookies = $cookieJar-\nremove(\nfoo.bar\n);\n\n// Remove the cookie named \nfoo\n for the domain \nfoo.bar\n\n$cookies = $cookieJar-\nremove(\nfoo.bar\n, \n/\n, \nfoo\n);\n\n\n\n\nRemove temporary cookies\n\n\n// Remove all temporary cookies\n$cookies = $cookieJar-\nremoveTemporary();\n\n\n\n\nRemove expired cookies\n\n\n// Remove all cookies that are expired\n$cookies = $cookieJar-\nremoveExpired();\n\n\n\n\nSerialize cookies\n\n\nCookies can be exported in a serializable format, thus it's possible to save the state and to\nuse it latter.\n\n\n// serialize cookies using json\n$serializableCookieJar = $cookieJar-\nexport();\n$serializedCookieJar = json_encode($serializableCookieJar);\n\n// Unserialize cookies in an other cookie jar\n$otherCookieJar-\nimport(json_decode($serializedCookieJar, true));", 
            "title": "Cookies"
        }, 
        {
            "location": "/cookies/#cookies", 
            "text": "Guide for cookie and cookiejar manipulation    SERPS offers convenient tools that emulate a cookiejar and allow to persist cookies across many requests.  The following examples introduce the work with cookies   Warning  Cookies usage is still at prototype stage and all http engines do not support cookies yet.", 
            "title": "Cookies"
        }, 
        {
            "location": "/cookies/#create-a-cookie", 
            "text": "Parameters for creating a cookie:   name : the name of a cookie.  value : the value of a cookie.  flags : cookie flags. Available flags are:  path  domain  expires  discard  secure     use Serps\\Core\\Cookie\\Cookie;\n\n$cookie = new Cookie('baz', 'bar', [\n    'domain' =  'foo.bar',\n    'path' =  '/',\n    'expires' =  time() + 1000,\n]);", 
            "title": "Create a cookie"
        }, 
        {
            "location": "/cookies/#populate-a-cookie-jar", 
            "text": "use Serps\\Core\\Cookie\\ArrayCookieJar;\n\n$cookieJar = new ArrayCookieJar();\n\n// Add the cookie 'foo' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('foo', 'bar', ['domain' =  'foo.bar']);\n$cookieJar- set($cookie);\n\n// Add the cookie 'baz' with value 'bar' for the domain 'foo.bar'\n$cookie = new Cookie('baz', 'bar', ['domain' =  'foo.bar']);\n$cookieJar- set($cookie);", 
            "title": "Populate a Cookie Jar"
        }, 
        {
            "location": "/cookies/#retrieves-cookies", 
            "text": "The  all  method is responsible for getting cookies matching some filters  Method parameters:   domain : filters the cookies matching the given domain. Pass null to match all domains.  path : filters the cookies matching the given path. Pass null to match all paths.  name : filters the cookies matching the given name. Pass null to match all names.  skipDiscardable : Set to TRUE to skip cookies with the Discard attribute. Default FALSE.  skipExpired : Set to FALSE to include expired. Default TRUE.   // Retrieves all cookies\n$cookies = $cookieJar- all();\n\n// Retrieves all cookies matching the domain  foo.bar \n$cookies = $cookieJar- all( foo.bar );\n\n\n// Retrieves the cookie named  foo  for the domain  foo.bar , including expired cookies\n$cookies = $cookieJar- all( foo.bar ,  / ,  foo , false, false);", 
            "title": "Retrieves cookies"
        }, 
        {
            "location": "/cookies/#retrieve-cookie-for-a-request", 
            "text": "It's possible to automatically retrieve cookies that match a given PSR-7 request:  // retrieves all cookies matching the request\n$cookies = $cookieJar- getMatchingCookies($request);", 
            "title": "Retrieve cookie for a request"
        }, 
        {
            "location": "/cookies/#remove-cookies", 
            "text": "// Remove all cookies\n$cookies = $cookieJar- remove();\n\n// Remove all cookies matching the domain  foo.bar \n$cookies = $cookieJar- remove( foo.bar );\n\n// Remove the cookie named  foo  for the domain  foo.bar \n$cookies = $cookieJar- remove( foo.bar ,  / ,  foo );", 
            "title": "Remove cookies"
        }, 
        {
            "location": "/cookies/#remove-temporary-cookies", 
            "text": "// Remove all temporary cookies\n$cookies = $cookieJar- removeTemporary();", 
            "title": "Remove temporary cookies"
        }, 
        {
            "location": "/cookies/#remove-expired-cookies", 
            "text": "// Remove all cookies that are expired\n$cookies = $cookieJar- removeExpired();", 
            "title": "Remove expired cookies"
        }, 
        {
            "location": "/cookies/#serialize-cookies", 
            "text": "Cookies can be exported in a serializable format, thus it's possible to save the state and to\nuse it latter.  // serialize cookies using json\n$serializableCookieJar = $cookieJar- export();\n$serializedCookieJar = json_encode($serializableCookieJar);\n\n// Unserialize cookies in an other cookie jar\n$otherCookieJar- import(json_decode($serializedCookieJar, true));", 
            "title": "Serialize cookies"
        }, 
        {
            "location": "/search-engine/google/", 
            "text": "Google Client\n\n\n\n\nEverything about the google client\n\n\n\n\n\n\nInstallation\n\n\nOverview\n\n\nCreate urls\n\n\nConfigure the google client\n\n\nParse a google page\n\n\nHandle errors\n\n\n\n\n\n\nInstallation\n\n\nThe google client is available with the package \n\nserps/search-engine-google\n: \n\n\n$ composer require 'serps/search-engine-google'\n\n\n\n\nImportant note agout google updates\n\n\nWe don't know when google dom updates, but when it does, we will place as many efforts as possible\nto get the client updated.\n\n\nTo make sure you are always up to date we advice you to \n\nwatch the repository on github\n.\nNot only you will be notified when a release is published, but you will also know when a bug is detected and \nreported on the issue tracker.\n\n\n\n\nOverview\n\n\nThe google client needs a http client interface to be constructed and an url to be parsed.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    // Create a google client using the curl http client\n    $googleClient = new GoogleClient(new CurlClient());\n\n    // Tell the client to use a user agent\n    $userAgent = \nMozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36\n;\n    $googleClient-\nrequest-\nsetUserAgent($userAgent);\n\n    // Create the url that will be parsed\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        // Analyse the results\n    }\n\n\n\n\n\n\nNext step: \nconfigure the client", 
            "title": "Google"
        }, 
        {
            "location": "/search-engine/google/#google-client", 
            "text": "Everything about the google client    Installation  Overview  Create urls  Configure the google client  Parse a google page  Handle errors", 
            "title": "Google Client"
        }, 
        {
            "location": "/search-engine/google/#installation", 
            "text": "The google client is available with the package  serps/search-engine-google :   $ composer require 'serps/search-engine-google'   Important note agout google updates  We don't know when google dom updates, but when it does, we will place as many efforts as possible\nto get the client updated.  To make sure you are always up to date we advice you to  watch the repository on github .\nNot only you will be notified when a release is published, but you will also know when a bug is detected and \nreported on the issue tracker.", 
            "title": "Installation"
        }, 
        {
            "location": "/search-engine/google/#overview", 
            "text": "The google client needs a http client interface to be constructed and an url to be parsed.      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    // Create a google client using the curl http client\n    $googleClient = new GoogleClient(new CurlClient());\n\n    // Tell the client to use a user agent\n    $userAgent =  Mozilla/5.0 (Windows NT 10.0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.93 Safari/537.36 ;\n    $googleClient- request- setUserAgent($userAgent);\n\n    // Create the url that will be parsed\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    $response = $googleClient- query($googleUrl);\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        // Analyse the results\n    }   Next step:  configure the client", 
            "title": "Overview"
        }, 
        {
            "location": "/http-client/curl/", 
            "text": "Curl HTTP Client\n\n\nCurl adapter for a simple scraping implementation\n\n\n\n\nCurl Adapter will allow to use the built in php CURL extension. This adapter \nfully supports proxies and cookies\n\n\nInstallation\n\n\nThe client is available with the package \n\nserps/http-client-curl\n: \n\n\n$ composer require 'serps/http-client-curl'\n\n\nAdditional requirement\n\n\nThis http client requires you to \ninstall the Curl php extension\n\n\nUsage\n\n\nuse Serps\\HttpClient\\PhantomJsClient;\n\n$client = new CurlClient();", 
            "title": "CURL"
        }, 
        {
            "location": "/http-client/curl/#curl-http-client", 
            "text": "Curl adapter for a simple scraping implementation   Curl Adapter will allow to use the built in php CURL extension. This adapter  fully supports proxies and cookies", 
            "title": "Curl HTTP Client"
        }, 
        {
            "location": "/http-client/curl/#installation", 
            "text": "The client is available with the package  serps/http-client-curl :   $ composer require 'serps/http-client-curl'", 
            "title": "Installation"
        }, 
        {
            "location": "/http-client/curl/#additional-requirement", 
            "text": "This http client requires you to  install the Curl php extension", 
            "title": "Additional requirement"
        }, 
        {
            "location": "/http-client/curl/#usage", 
            "text": "use Serps\\HttpClient\\PhantomJsClient;\n\n$client = new CurlClient();", 
            "title": "Usage"
        }, 
        {
            "location": "/http-client/phantomJS/", 
            "text": "PhantomJS HTTP Client\n\n\n\n\nPhantomJS\n is a webkit implementation that helps to simulate the real browser.\n\n\n\n\nBy using this client you will execute the inner javascript code and make the DOM as real as in the true browser,\nthat can be required for some search engines to work properly.\n\n\n\n\nNotice about cookies\n\n\nAt the current state phantomJS adapter does not support internal cookieJar usage.\n\n\n\n\nInstallation\n\n\nThe client is available with the package \n\nserps/http-client-phantomjs\n: \n\n\n$ composer require 'serps/http-client-phantomjs'\n\n\nAdditional requirement\n\n\nPhantomJS\n binaries have to be installed\n to use the client. The process for installing\nit depends on your environment, you will find further guides on the internet.\n\n\n\n\nNote\n\n\nThe package \njakoch/phantomjs-installer\n \ncan help you to manage phantomJS as a dependency of your project.\n\n\n\n\nUsage\n\n\nuse Serps\\HttpClient\\PhantomJsClient;\n\n// The constructor accepts 1 optional parameter that is the path to the phantomjs binaries (default to 'phantomjs')\n$client = new PhantomJsClient();", 
            "title": "PhantomJS"
        }, 
        {
            "location": "/http-client/phantomJS/#phantomjs-http-client", 
            "text": "PhantomJS  is a webkit implementation that helps to simulate the real browser.   By using this client you will execute the inner javascript code and make the DOM as real as in the true browser,\nthat can be required for some search engines to work properly.   Notice about cookies  At the current state phantomJS adapter does not support internal cookieJar usage.", 
            "title": "PhantomJS HTTP Client"
        }, 
        {
            "location": "/http-client/phantomJS/#installation", 
            "text": "The client is available with the package  serps/http-client-phantomjs :   $ composer require 'serps/http-client-phantomjs'", 
            "title": "Installation"
        }, 
        {
            "location": "/http-client/phantomJS/#additional-requirement", 
            "text": "PhantomJS  binaries have to be installed  to use the client. The process for installing\nit depends on your environment, you will find further guides on the internet.   Note  The package  jakoch/phantomjs-installer  \ncan help you to manage phantomJS as a dependency of your project.", 
            "title": "Additional requirement"
        }, 
        {
            "location": "/http-client/phantomJS/#usage", 
            "text": "use Serps\\HttpClient\\PhantomJsClient;\n\n// The constructor accepts 1 optional parameter that is the path to the phantomjs binaries (default to 'phantomjs')\n$client = new PhantomJsClient();", 
            "title": "Usage"
        }, 
        {
            "location": "/http-client/spidyJS/", 
            "text": "SpidyJS HTTP Client\n\n\nSpidy\n is a browser built with javascript. \n\n\n\n\nThis adapter allows you to query search engines with spidyJS, it is a javascript headless browser. \n\n\n\n\nWarning\n\n\nThis adapter is still a prototype. Use it with care.\n\n\n\n\nInstallation\n\n\nThe client is available with the package \n\nserps/http-client-spidyjs\n: \n\n\n$ composer require 'serps/http-client-spidyjs'\n\n\nAdditional requirement\n\n\nYou will also need nodejs and npm to install spidy.\n\n\n$ npm install -g spidy@2\n\n\n\n\nIf you use a nodejs version that is before 4.0, you will need to install spidy version 1 instead:\n\n\n$ npm install -g spidy@1\n\n\n\n\nUsage\n\n\nuse Serps\\HttpClient\\SpidyJsClient;\n\n\n// The constructor accepts 1 optional parameter that is the path to the spidyjs binaries (default to 'spidyjs')\n$client = new SpidyJsClient();", 
            "title": "SpidyJS"
        }, 
        {
            "location": "/http-client/spidyJS/#spidyjs-http-client", 
            "text": "Spidy  is a browser built with javascript.    This adapter allows you to query search engines with spidyJS, it is a javascript headless browser.    Warning  This adapter is still a prototype. Use it with care.", 
            "title": "SpidyJS HTTP Client"
        }, 
        {
            "location": "/http-client/spidyJS/#installation", 
            "text": "The client is available with the package  serps/http-client-spidyjs :   $ composer require 'serps/http-client-spidyjs'", 
            "title": "Installation"
        }, 
        {
            "location": "/http-client/spidyJS/#additional-requirement", 
            "text": "You will also need nodejs and npm to install spidy.  $ npm install -g spidy@2  If you use a nodejs version that is before 4.0, you will need to install spidy version 1 instead:  $ npm install -g spidy@1", 
            "title": "Additional requirement"
        }, 
        {
            "location": "/http-client/spidyJS/#usage", 
            "text": "use Serps\\HttpClient\\SpidyJsClient;\n\n\n// The constructor accepts 1 optional parameter that is the path to the spidyjs binaries (default to 'spidyjs')\n$client = new SpidyJsClient();", 
            "title": "Usage"
        }, 
        {
            "location": "/about/packages/", 
            "text": "Packages\n\n\nList of packages that are part of this project\n\n\n\n\nCore\n\n\nThis is the core of SERPS. It contains the common tools that are used by search engine and http client implementations\n\n\n Github\n\n\n$ composer require serps/serps\n\n\n\n\n\n\n\n\n\n\nSearch engines\n\n\nGoogle\n\n\nThe google client implementation\n\n\n Github\n\n\n$ composer require serps/search-engine-google\n\n\n\n\n\n\n\n\n\n\nHttp clients\n\n\nCurl\n\n\nCurl Http client\n\n\n Doc\n\n\n Github\n\n\n$ composer require serps/http-client-curl\n\n\n\n\n\n\n\n\nPhantomJS\n\n\nPhantomJS Http client\n\n\n Doc\n\n\n Github\n\n\n$ composer require serps/http-client-phantomjs\n\n\n\n\n\n\n\n\n\n\nSpidyJS\n\n\nnodejs browser built specially for SERPS\n\n\n Github\n\n\n$ npm install spidyjs\n\n\n\nmaster (node \n= 4):\n\n\n1.x (node \n 4):\n\n\n\nWebsite homepage\n\n\nSimply the homepage at https://serp-spider.github.io/\n\n\n Github\n\n\nDocumentation\n\n\nThe package that contains this documentation at http://serp-spider.github.io/documentation\n\n\n Github\n\n\nStatus monitor\n\n\nCli application that helps to monitor search engine changes\n\n\n Github", 
            "title": "Packages"
        }, 
        {
            "location": "/about/packages/#packages", 
            "text": "List of packages that are part of this project", 
            "title": "Packages"
        }, 
        {
            "location": "/about/packages/#core", 
            "text": "This is the core of SERPS. It contains the common tools that are used by search engine and http client implementations   Github  $ composer require serps/serps", 
            "title": "Core"
        }, 
        {
            "location": "/about/packages/#search-engines", 
            "text": "", 
            "title": "Search engines"
        }, 
        {
            "location": "/about/packages/#google", 
            "text": "The google client implementation   Github  $ composer require serps/search-engine-google", 
            "title": "Google"
        }, 
        {
            "location": "/about/packages/#http-clients", 
            "text": "", 
            "title": "Http clients"
        }, 
        {
            "location": "/about/packages/#curl", 
            "text": "Curl Http client   Doc   Github  $ composer require serps/http-client-curl", 
            "title": "Curl"
        }, 
        {
            "location": "/about/packages/#phantomjs", 
            "text": "PhantomJS Http client   Doc   Github  $ composer require serps/http-client-phantomjs", 
            "title": "PhantomJS"
        }, 
        {
            "location": "/about/packages/#spidyjs", 
            "text": "nodejs browser built specially for SERPS   Github  $ npm install spidyjs  \nmaster (node  = 4): \n1.x (node   4):", 
            "title": "SpidyJS"
        }, 
        {
            "location": "/about/packages/#website-homepage", 
            "text": "Simply the homepage at https://serp-spider.github.io/   Github", 
            "title": "Website homepage"
        }, 
        {
            "location": "/about/packages/#documentation", 
            "text": "The package that contains this documentation at http://serp-spider.github.io/documentation   Github", 
            "title": "Documentation"
        }, 
        {
            "location": "/about/packages/#status-monitor", 
            "text": "Cli application that helps to monitor search engine changes   Github", 
            "title": "Status monitor"
        }, 
        {
            "location": "/search-engine/google/parse-page/", 
            "text": "Parse a Google Page\n\n\n\n\nThe necessary documentation about parsing a google page\n\n\n\n\nBack to the \ngeneral google documentation\n.\n\n\n\n\n\n\nImportant notice about google update\n\n\nThe following examples can change at any time. \n\n\nAs soon as google changes its page structure, you may need to update the library.\n\n\n\n\n\n\nA google SERP can contain different type of result.\nFirstly they are divided in three distinct regions: \nnatural\n (organic), \npaid\n (adwords) and \ngraph results\n and each of them\nhas its own results types. Graph result are \nnot supported\n by the library.\n\n\nThrough there is a great diversity of results the library gives you the api to work with them, here we document\nhow to work with it.\n\n\nNatural Results\n\n\nNatural results (aka organic results) are main results of the page.\n\n\nEach natural result has a position and some available data. You can access them the following way (see the foreach loop):\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleClient = new GoogleClient($httpClient);\n\n    $googleUrl = new GoogleUrl();\n    $google-\nsetSearchTerm('simpsons');\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        // Here we iterate over the result list\n        // Each result will have different data based on its type\n    }\n\n\n\n\nEach of the result from the loop will have the following methods available:\n\n\n\n\ngetTypes()\n: the types of the result\n\n\nis($type)\n: check if the result is of the given type\n\n\ngetDataValue($type)\n: Get the given data from the result. Everything accessible with \ngetDataValue\n\nis also accessible with a property, e.g the two examples do the same thing: \n$result-\ngetDataValue('url')\n and \n$result-\nurl\n\n\ngetData()\n: Get the all the data of the result\n\n\ngetOnPagePosition()\n: Get the position of the result on the page (not aware of the pagination)\n\n\ngetRealPosition()\n: Get the global position of the result (aware of the pagination)\n\n\n\n\nThe difference between each result type is the list of data available with \ngetDataValue($type)\n and \ngetData()\n.\nSee bellow for all available data per result type.\n\n\nNatural Result Types\n\n\nResult types\n can be accessed through the class \nNaturalResultType\n,\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    if($result-\nis(NaturalResultType::CLASSICAL)){\n        // Do stuff\n    }\n\n    // You can also check many types at once\n    // Here we check if the result is classical or image group\n\n    if($result-\nis(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP)){\n        // Do stuff\n    }\n\n\n\n\nFrom the \nresultSet\n you can also access all the results matching one of the given type:\n\n\n    // Get all the results that are either classical or image_group\n    $results = $results-\ngetResultsByType(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP);\n\n\n\n\nClassical\n\n\nThese results are the common natural results that have always existed in google.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::CLASSICAL\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n [\nA\n]\n\n\nurl\n \nstring\n: the url targeted on clicking the title\n\n\ndestination\n \nstring\n [\nB\n]: either a url or a breadcrumb-like destination\n\n\ndescription\n \nstring\n [\nC\n]\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::CLASSICAL)){\n            $title = $result-\ntitle;\n            $url   = $result-\nurl;\n        }\n    }\n\n\n\n\nClassical Video\n\n\nThis type an extension of the \nclassical result\n, but it refers to a video result.\n\n\nThe video result can be illustrated with either a thumbnail or a large image.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::CLASSICAL_VIDEO\n\n\nNaturalResultType::CLASSICAL\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n [\nA\n]\n\n\nurl\n \nstring\n: the url targeted on clicking the title\n\n\ndestination\n \nstring\n [\nB\n]: either a url or a breadcrumb-like destination\n\n\ndescription\n \nstring\n [\nC\n]\n\n\nvideoLarge\n \nbool\n: true if the video is image is large (usually first result)\n\n\nvideoCover\n \nstring\n: the video picture as given by google - either an image url or a base64 encoded image\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::CLASSICAL_VIDEO)){\n            $title = $result-\ntitle;\n            if($result-\nvideoLarge){\n                // ...\n            }\n        }\n    }\n\n\n\n\nImage Group\n\n\nImages that appear as a group of results.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::IMAGE_GROUP\n\n\n\n\nData\n\n\n\n\nimages\n \narray\n: the list of images that compose the image group, each image contains:\n\n\nsourceUrl\n \nUrl\n: the url where the image was found\n\n\ntargetUrl\nUrl\n: the url reached on clicking the image\n\n\nimage\n \nstring\n: the image data as specified by google (either an image url or a base64 encoded image)\n\n\n\n\n\n\nmoreUrl\n \nUrl\n: The url corresponding to the google image search\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::IMAGE_GROUP)){\n            foreach($result-\nimages as $image){\n                $sourceUrl = $image-\nsourceUrl;\n            }\n        }\n    }\n\n\n\n\nMap\n\n\nA result illustrated by a map and that contains sub-results.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::MAP\n\n\n\n\nData\n\n\n\n\nlocalPack\n \narray\n: The sub results for the map:\n\n\ntitle\n \nstring\n \n[A]\n: Name of the place\n\n\nurl\nUrl\n \n[B]\n: Website of the sub-result\n\n\nstreet\n \nstring\n \n[C]\n: The address of the sub-result\n\n\nstars\n \nstring\n \n[D]\n: The rating of the result as a number\n\n\nreview\n \nstring\n \n[E]\n: The review string as specified by google (e.g '1 review')\n\n\nphone\n \nstring\n \n[G]\n: The phone number\n\n\n\n\n\n\nmapUrl\n \nUrl\n \n[F]\n: The url to access the map search\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::MAP)){\n            foreach($result-\nlocalPack as $place){\n                $website = $place-\nwebsite;\n            }\n        }\n    }\n\n\n\n\nTweet Carousel\n\n\nRecent tweet list from an user matching the search keywords.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::TWEETS_CAROUSEL\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n \n[A]\n\n\nurl\n \nstring\n: The url reach when clicking the title\n\n\nuser\nUrl\n: The author of the tweets\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::TWEETS_CAROUSEL)){\n            $user = $result-\nuser;\n        }\n    }\n\n\n\n\nIn the News\n\n\nRecent news results.\n\n\n\n\nAvailable with\n\n\n\n\nNaturalResultType::IN_THE_NEWS\n\n\n\n\nData\n\n\n\n\nnews\n \narray\n\n\ntitle\n \nstring\n \n[A]\n\n\ndescription\n \nUrl\n \n[B]\n\n\nurl\nstring\n: The url reached when clicking the title\n\n\n\n\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response-\ngetNaturalResults();\n\n    foreach($results as $result){\n        if($result-\nis(NaturalResultType::IN_THE_NEWS)){\n            $title = $result-\ntitle;\n        }\n    }\n\n\n\n\nAdwords Results\n\n\nThe google client offers an Adwords parser.\n\n\n\n\nWarning\n\n\nAdwords parsing is still experimental!\n\n\n\n\n    $adwordsResults = $response-\ngetAdwordsResults();\n\n    foreach($results as $result){\n        // do stuff\n    }\n\n\n\n\nAdwords sections\n\n\nAdwords results are composed from 3 distinct sections. These sections can be at the top, at the right or at the bottom\nof the natural results. See the schema:\n\n\n\n\nBy default all results are available in the result set, if you need \nto get results from a section, you can use the section as a type filter:\n\n\n    use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $adwordsResults = $response-\ngetAdwordsResults();\n\n    $topResults = $adwordsResults-\ngetResultsByType(AdwordsResultType::SECTION_TOP);\n    $rightResults = $adwordsResults-\ngetResultsByType(AdwordsResultType::SECTION_RIGHT);\n    $bottomResults = $adwordsResults-\ngetResultsByType(AdwordsResultType::SECTION_BOTTOM);\n\n    foreach($topResults as $result){\n        // Do stuff...\n    }\n\n\n\n\nAdwords Types\n\n\nAd\n\n\nAds results are the basics results from adwords.\n\n\n\n\nAvailable with\n\n\n\n\nAdwordsResultType::AD\n\n\n\n\nData\n\n\n\n\ntitle\n \nstring\n \n[A]\n\n\nurl\n \nurl\n: The url reach when clicking the title\n\n\nvisurl\n \nstring\n \n[B]\n: The visual url\n\n\ndescription\nstring\n \n[C]\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n\n    $results = $response-\ngetAdwordsResults();\n\n    foreach($results as $result){\n        if($result-\nis(AdwordsResultType::AD)){\n            $url = $result-\nurl;\n        }\n    }\n\n\n\n\nShopping\n\n\nThese are the results from google shopping/merchant.\n\n\n\n\nAvailable with\n\n\n\n\nAdwordsResultType::SHOPPING_GROUP\n\n\n\n\nData\n\n\n\n\nproducts\n \narray\n: The product list. Each product contains the following items:\n\n\ntitle\n \nstring\n \n[A]\n\n\nimage\n \nstring\n \n[B]\n: the image as specified by google - either an image url or a base64 encoded image\n\n\nurl\n \nurl\n: The url reached when clicking the title\n\n\ntarget\n \nstring\n \n[C]\n: The target website as shown by google\n\n\nprice\nstring\n \n[D]\n: The price as show by google\n\n\n\n\n\n\n\n\nExample\n\n\n    use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $results = $response-\ngetAdwordsResults();\n\n    foreach($results as $result){\n        if($result-\nis(AdwordsResultType::SHOPPING_GROUP)){\n            foreach($result-\nproducts as $item){\n                $title = $item-\ntitle;\n            }\n        }\n    }\n\n\n\n\nAdditional info\n\n\nA Google SERP contains even more information that the result list. Sometime they will be very helpful to get the\nmost from the SERP.\n\n\nHere is the list of these info currently supported by the parser.\n\n\nNumber of results\n\n\n\n\nRepresents the total number of results returned by the current search. \nThe format of this number can change from country to country (61,000,000 or 61 000 000 or 6,10,00,000 etc...) \nWe take care of returning this number as a integer no matter the initial format.\n\n\nIf \nnull\n is returned, it means that the parsing failed. In this case check that your client is up to date, or else \n\nopen an issue\n.\n\n\n    $numberOfResults = $response-\ngetNumberOfResults();\n\n    if(null === $numberOfResults){\n        // houston, we have a problem...\n    } elseif($numberOfResults \n 2000) {\n        // not so many\n    } else {\n        // a way too much\n    }\n\n\n\n\nRelated searches\n\n\nNot implemented yet.\n\n\nCustom parsing\n\n\nSometimes you need information that are not available in our parser. \n\n\nFirst of all, search if someone already asked for this feature \non the \nissue tracker\n. \n\n\nIf you don't find a trace of this feature, but you still consider that this feature is important, then open an issue and\nlet's discuss it. This is very important because if the feature is implemented in the library it will take advantage of\nbeing updated on google updates, and you wont have to maintain it.\n\n\n\n\nBack from the issue tracker, no one mentioned it and you still \nwant to parse the information by yourself\n.\n Alright, here are the tools you need.\n\n\nQuery with css\n\n\nThe easiest way to do it for a web developer: \nwith css\n.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    // Returns \\DOMNodeList\n    $queryResult = $response-\ncssQuery('#someId');\n\n    if ($queryResult-\nlength == 1) {\n        // You can query again to find items in the previous context.\n\n        // Gets all items with the class 'someClass' within the element with the id 'someId'\n        $queryResult = $response-\ncssQuery('.someClass', $queryResult-\nitem(0));\n    } else {\n        // some errors...\n    }\n\n\n\n\nIt works exactly as \nDOMXPath::query\n does. Actually the css is translated \nto xpath and \nDOMXPath::query\n is called on the dom element.\n\n\nQuery with xpath\n\n\nThat's very similar to the css way, except that you will use \nxpath\n.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $queryResult = $response-\ncssQuery('descendant::div[@id=\nsomeId\n]');\n\n    if ($queryResult-\nlength == 1) {\n        // Gets all 'a' tags inside the element with the id 'someId'.\n        $queryResult = $response-\ncssQuery('a', $queryResult-\nitem(0));\n    } else {\n        // some errors...\n    }\n\n\n\n\nThere is also a shortcut to the xpath object.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $xpath = $response-\ngetXpath();\n    $xpath-\nquery('someXpath');\n\n\n\n\nManipulate the DOM object\n\n\nYou can get the \nDOM object\n to manipulate it, or to save it in a file.\n\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    $dom = $response-\ngetDom();\n\n    // Writes the dom content in the file 'file.html'\n    $dom-\nsave('file.html');", 
            "title": "Parse a Google Page"
        }, 
        {
            "location": "/search-engine/google/parse-page/#parse-a-google-page", 
            "text": "The necessary documentation about parsing a google page   Back to the  general google documentation .    Important notice about google update  The following examples can change at any time.   As soon as google changes its page structure, you may need to update the library.    A google SERP can contain different type of result.\nFirstly they are divided in three distinct regions:  natural  (organic),  paid  (adwords) and  graph results  and each of them\nhas its own results types. Graph result are  not supported  by the library.  Through there is a great diversity of results the library gives you the api to work with them, here we document\nhow to work with it.", 
            "title": "Parse a Google Page"
        }, 
        {
            "location": "/search-engine/google/parse-page/#natural-results", 
            "text": "Natural results (aka organic results) are main results of the page.  Each natural result has a position and some available data. You can access them the following way (see the foreach loop):      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleClient = new GoogleClient($httpClient);\n\n    $googleUrl = new GoogleUrl();\n    $google- setSearchTerm('simpsons');\n\n    $response = $googleClient- query($googleUrl);\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        // Here we iterate over the result list\n        // Each result will have different data based on its type\n    }  Each of the result from the loop will have the following methods available:   getTypes() : the types of the result  is($type) : check if the result is of the given type  getDataValue($type) : Get the given data from the result. Everything accessible with  getDataValue \nis also accessible with a property, e.g the two examples do the same thing:  $result- getDataValue('url')  and  $result- url  getData() : Get the all the data of the result  getOnPagePosition() : Get the position of the result on the page (not aware of the pagination)  getRealPosition() : Get the global position of the result (aware of the pagination)   The difference between each result type is the list of data available with  getDataValue($type)  and  getData() .\nSee bellow for all available data per result type.", 
            "title": "Natural Results"
        }, 
        {
            "location": "/search-engine/google/parse-page/#natural-result-types", 
            "text": "Result types  can be accessed through the class  NaturalResultType ,      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    if($result- is(NaturalResultType::CLASSICAL)){\n        // Do stuff\n    }\n\n    // You can also check many types at once\n    // Here we check if the result is classical or image group\n\n    if($result- is(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP)){\n        // Do stuff\n    }  From the  resultSet  you can also access all the results matching one of the given type:      // Get all the results that are either classical or image_group\n    $results = $results- getResultsByType(NaturalResultType::CLASSICAL, NaturalResultType::IMAGE_GROUP);", 
            "title": "Natural Result Types"
        }, 
        {
            "location": "/search-engine/google/parse-page/#classical", 
            "text": "These results are the common natural results that have always existed in google.   Available with   NaturalResultType::CLASSICAL   Data   title   string  [ A ]  url   string : the url targeted on clicking the title  destination   string  [ B ]: either a url or a breadcrumb-like destination  description   string  [ C ]   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::CLASSICAL)){\n            $title = $result- title;\n            $url   = $result- url;\n        }\n    }", 
            "title": "Classical"
        }, 
        {
            "location": "/search-engine/google/parse-page/#classical-video", 
            "text": "This type an extension of the  classical result , but it refers to a video result.  The video result can be illustrated with either a thumbnail or a large image.   Available with   NaturalResultType::CLASSICAL_VIDEO  NaturalResultType::CLASSICAL   Data   title   string  [ A ]  url   string : the url targeted on clicking the title  destination   string  [ B ]: either a url or a breadcrumb-like destination  description   string  [ C ]  videoLarge   bool : true if the video is image is large (usually first result)  videoCover   string : the video picture as given by google - either an image url or a base64 encoded image   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::CLASSICAL_VIDEO)){\n            $title = $result- title;\n            if($result- videoLarge){\n                // ...\n            }\n        }\n    }", 
            "title": "Classical Video"
        }, 
        {
            "location": "/search-engine/google/parse-page/#image-group", 
            "text": "Images that appear as a group of results.   Available with   NaturalResultType::IMAGE_GROUP   Data   images   array : the list of images that compose the image group, each image contains:  sourceUrl   Url : the url where the image was found  targetUrl Url : the url reached on clicking the image  image   string : the image data as specified by google (either an image url or a base64 encoded image)    moreUrl   Url : The url corresponding to the google image search   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::IMAGE_GROUP)){\n            foreach($result- images as $image){\n                $sourceUrl = $image- sourceUrl;\n            }\n        }\n    }", 
            "title": "Image Group"
        }, 
        {
            "location": "/search-engine/google/parse-page/#map", 
            "text": "A result illustrated by a map and that contains sub-results.   Available with   NaturalResultType::MAP   Data   localPack   array : The sub results for the map:  title   string   [A] : Name of the place  url Url   [B] : Website of the sub-result  street   string   [C] : The address of the sub-result  stars   string   [D] : The rating of the result as a number  review   string   [E] : The review string as specified by google (e.g '1 review')  phone   string   [G] : The phone number    mapUrl   Url   [F] : The url to access the map search   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::MAP)){\n            foreach($result- localPack as $place){\n                $website = $place- website;\n            }\n        }\n    }", 
            "title": "Map"
        }, 
        {
            "location": "/search-engine/google/parse-page/#tweet-carousel", 
            "text": "Recent tweet list from an user matching the search keywords.   Available with   NaturalResultType::TWEETS_CAROUSEL   Data   title   string   [A]  url   string : The url reach when clicking the title  user Url : The author of the tweets   Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::TWEETS_CAROUSEL)){\n            $user = $result- user;\n        }\n    }", 
            "title": "Tweet Carousel"
        }, 
        {
            "location": "/search-engine/google/parse-page/#in-the-news", 
            "text": "Recent news results.   Available with   NaturalResultType::IN_THE_NEWS   Data   news   array  title   string   [A]  description   Url   [B]  url string : The url reached when clicking the title     Example      use Serps\\SearchEngine\\Google\\NaturalResultType;\n\n\n    $results = $response- getNaturalResults();\n\n    foreach($results as $result){\n        if($result- is(NaturalResultType::IN_THE_NEWS)){\n            $title = $result- title;\n        }\n    }", 
            "title": "In the News"
        }, 
        {
            "location": "/search-engine/google/parse-page/#adwords-results", 
            "text": "The google client offers an Adwords parser.   Warning  Adwords parsing is still experimental!       $adwordsResults = $response- getAdwordsResults();\n\n    foreach($results as $result){\n        // do stuff\n    }", 
            "title": "Adwords Results"
        }, 
        {
            "location": "/search-engine/google/parse-page/#adwords-sections", 
            "text": "Adwords results are composed from 3 distinct sections. These sections can be at the top, at the right or at the bottom\nof the natural results. See the schema:   By default all results are available in the result set, if you need \nto get results from a section, you can use the section as a type filter:      use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $adwordsResults = $response- getAdwordsResults();\n\n    $topResults = $adwordsResults- getResultsByType(AdwordsResultType::SECTION_TOP);\n    $rightResults = $adwordsResults- getResultsByType(AdwordsResultType::SECTION_RIGHT);\n    $bottomResults = $adwordsResults- getResultsByType(AdwordsResultType::SECTION_BOTTOM);\n\n    foreach($topResults as $result){\n        // Do stuff...\n    }", 
            "title": "Adwords sections"
        }, 
        {
            "location": "/search-engine/google/parse-page/#adwords-types", 
            "text": "", 
            "title": "Adwords Types"
        }, 
        {
            "location": "/search-engine/google/parse-page/#ad", 
            "text": "Ads results are the basics results from adwords.   Available with   AdwordsResultType::AD   Data   title   string   [A]  url   url : The url reach when clicking the title  visurl   string   [B] : The visual url  description string   [C]   Example      use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n\n    $results = $response- getAdwordsResults();\n\n    foreach($results as $result){\n        if($result- is(AdwordsResultType::AD)){\n            $url = $result- url;\n        }\n    }", 
            "title": "Ad"
        }, 
        {
            "location": "/search-engine/google/parse-page/#shopping", 
            "text": "These are the results from google shopping/merchant.   Available with   AdwordsResultType::SHOPPING_GROUP   Data   products   array : The product list. Each product contains the following items:  title   string   [A]  image   string   [B] : the image as specified by google - either an image url or a base64 encoded image  url   url : The url reached when clicking the title  target   string   [C] : The target website as shown by google  price string   [D] : The price as show by google     Example      use Serps\\SearchEngine\\Google\\AdwordsResultType;\n\n    $results = $response- getAdwordsResults();\n\n    foreach($results as $result){\n        if($result- is(AdwordsResultType::SHOPPING_GROUP)){\n            foreach($result- products as $item){\n                $title = $item- title;\n            }\n        }\n    }", 
            "title": "Shopping"
        }, 
        {
            "location": "/search-engine/google/parse-page/#additional-info", 
            "text": "A Google SERP contains even more information that the result list. Sometime they will be very helpful to get the\nmost from the SERP.  Here is the list of these info currently supported by the parser.", 
            "title": "Additional info"
        }, 
        {
            "location": "/search-engine/google/parse-page/#number-of-results", 
            "text": "Represents the total number of results returned by the current search. \nThe format of this number can change from country to country (61,000,000 or 61 000 000 or 6,10,00,000 etc...) \nWe take care of returning this number as a integer no matter the initial format.  If  null  is returned, it means that the parsing failed. In this case check that your client is up to date, or else  open an issue .      $numberOfResults = $response- getNumberOfResults();\n\n    if(null === $numberOfResults){\n        // houston, we have a problem...\n    } elseif($numberOfResults   2000) {\n        // not so many\n    } else {\n        // a way too much\n    }", 
            "title": "Number of results"
        }, 
        {
            "location": "/search-engine/google/parse-page/#related-searches", 
            "text": "Not implemented yet.", 
            "title": "Related searches"
        }, 
        {
            "location": "/search-engine/google/parse-page/#custom-parsing", 
            "text": "Sometimes you need information that are not available in our parser.   First of all, search if someone already asked for this feature \non the  issue tracker .   If you don't find a trace of this feature, but you still consider that this feature is important, then open an issue and\nlet's discuss it. This is very important because if the feature is implemented in the library it will take advantage of\nbeing updated on google updates, and you wont have to maintain it.   Back from the issue tracker, no one mentioned it and you still  want to parse the information by yourself .\n Alright, here are the tools you need.", 
            "title": "Custom parsing"
        }, 
        {
            "location": "/search-engine/google/parse-page/#query-with-css", 
            "text": "The easiest way to do it for a web developer:  with css .      $response = $googleClient- query($googleUrl);\n\n    // Returns \\DOMNodeList\n    $queryResult = $response- cssQuery('#someId');\n\n    if ($queryResult- length == 1) {\n        // You can query again to find items in the previous context.\n\n        // Gets all items with the class 'someClass' within the element with the id 'someId'\n        $queryResult = $response- cssQuery('.someClass', $queryResult- item(0));\n    } else {\n        // some errors...\n    }  It works exactly as  DOMXPath::query  does. Actually the css is translated \nto xpath and  DOMXPath::query  is called on the dom element.", 
            "title": "Query with css"
        }, 
        {
            "location": "/search-engine/google/parse-page/#query-with-xpath", 
            "text": "That's very similar to the css way, except that you will use  xpath .      $response = $googleClient- query($googleUrl);\n\n    $queryResult = $response- cssQuery('descendant::div[@id= someId ]');\n\n    if ($queryResult- length == 1) {\n        // Gets all 'a' tags inside the element with the id 'someId'.\n        $queryResult = $response- cssQuery('a', $queryResult- item(0));\n    } else {\n        // some errors...\n    }  There is also a shortcut to the xpath object.      $response = $googleClient- query($googleUrl);\n\n    $xpath = $response- getXpath();\n    $xpath- query('someXpath');", 
            "title": "Query with xpath"
        }, 
        {
            "location": "/search-engine/google/parse-page/#manipulate-the-dom-object", 
            "text": "You can get the  DOM object  to manipulate it, or to save it in a file.      $response = $googleClient- query($googleUrl);\n\n    $dom = $response- getDom();\n\n    // Writes the dom content in the file 'file.html'\n    $dom- save('file.html');", 
            "title": "Manipulate the DOM object"
        }, 
        {
            "location": "/search-engine/google/google-url/", 
            "text": "Work with google urls\n\n\nSERPS gives you the tools you need to create urls for google.\n\n\n\n\nBack to the \ngeneral google documentation\n.\n\n\n\n\nThe class \nGoogleUrl\n offers many convenient tools to work with google urls. See how it works with examples.\n\n\nCreate an url\n\n\nThe url builder has the required tools to build an url from scratch.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n    $googleUrl-\nsetLanguageRestriction('lang_en');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\nlr=lang_en\n\n\n\n\nUrl from a string\n\n\nIt's also possible to parse an an existing google url string to an url object.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = GoogleUrl::fromString('https://google.com/search?q=simpsons');\n    echo $googleUrl-\ngetSearchTerm();\n    // simpsons\n\n\n\n\nAdditionally you can continue to manipulate this url\n\n\n    $googleUrl-\nsetLanguageRestriction('lang_en');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\nlr=lang_en\n\n\n\n\nGoogle domain\n\n\nBy default an url is generated for \ngoogle.com\n but you can choose any domain of your choice:\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl('google.fr');\n    $googleUrl-\nsetSearchTerm('simpsons');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.fr/search?q=simpsons\n\n\n\n\nIt's also possible to modify it latter\n\n\n    $googleUrl-\nsetHost('google.de');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.de/search?q=simpsons\n\n\n\n\nAdd and remove parameters\n\n\nIt's possible to add or remove parameters\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetParam('q', 'simpsons');\n    $googleUrl-\nsetParam('start', 11);\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\nstart=11\n\n    $googleUrl-\nremoveParam('start');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=simpsons\n\n\n\n\nRaw parameters\n\n\nBy default parameters are encoded for urls. For instance \n\"Homer Simpsons\"\n will become \n\"Homer+Simpsons\"\n\nbut \n\"Homer+Simpsons\"\n will become \n\"Homer%2BSimpson\"\n\n\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetParam('q', 'Homer+Simpson');\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=Homer%2BSimpson\n\n\n\n\nIt's possible to deal with raw params, this way the param will be passed to the url with no additional encoding. \nThat is achieved by passing true as the third argument of \nsetParam\n.\n\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetParam('q', 'Homer+Simpson', true);\n    echo $googleUrl-\nbuildUrl();\n    // https://google.com/search?q=Homer+Simpson\n\n\n\n\nMore parameters\n\n\nSome parameters are very common and for some of them we created convenient shortcuts. See the list:\n\n\nsetSearchTerm\n\n\n$url-\nsetSearchTerm($searchTerm)\n\n\nSets the keywords to search for. Modifies the value for the \nq\n parameter.\n\n\nsetPage\n\n\n$url-\nsetPage($pageNumber)\n\n\nSets the page to parse (starts at 0). Modifies the value for the \nstart\n parameter.\n\n\n\n\nNote\n\n\nIf value is less than 0, then the param \nstart\n will be removed from the url\n\n\n\n\nsetResultsPerPage\n\n\n$url-\nsetResultsPerPage($resultsPerPages)\n\n\nSets the number of results per pages (between 1 and 100). Modifies the value for the \nnum\n parameter.\n\n\nsetLanguageRestriction\n\n\n$url-\nsetLanguageRestriction($lang)\n\n\nSets language of the results. Modifies the value for the \nlr\n parameter. e.g  \n\"lang_en\"\n. \n\n\n\n\nNote\n\n\n\"lang_\"\n will be automatically prepended if it is not present. \n\n\nThat means that \n$url-\nsetLanguageRestriction('en')\n\nand \n$url-\nsetLanguageRestriction('lang_en')\n do the same thing.\n\n\n\n\nsetAutoCorrectionEnabled\n\n\n$url-\nsetAutoCorrectionEnabled($enabled)\n\n\nSets if the auto correction should be enabled (\ntrue\n or \nfalse\n). Modifies the value for the \nnfpr\n parameter.", 
            "title": "Google Url"
        }, 
        {
            "location": "/search-engine/google/google-url/#work-with-google-urls", 
            "text": "SERPS gives you the tools you need to create urls for google.   Back to the  general google documentation .   The class  GoogleUrl  offers many convenient tools to work with google urls. See how it works with examples.", 
            "title": "Work with google urls"
        }, 
        {
            "location": "/search-engine/google/google-url/#create-an-url", 
            "text": "The url builder has the required tools to build an url from scratch.      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n    $googleUrl- setLanguageRestriction('lang_en');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons lr=lang_en", 
            "title": "Create an url"
        }, 
        {
            "location": "/search-engine/google/google-url/#url-from-a-string", 
            "text": "It's also possible to parse an an existing google url string to an url object.      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = GoogleUrl::fromString('https://google.com/search?q=simpsons');\n    echo $googleUrl- getSearchTerm();\n    // simpsons  Additionally you can continue to manipulate this url      $googleUrl- setLanguageRestriction('lang_en');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons lr=lang_en", 
            "title": "Url from a string"
        }, 
        {
            "location": "/search-engine/google/google-url/#google-domain", 
            "text": "By default an url is generated for  google.com  but you can choose any domain of your choice:      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl('google.fr');\n    $googleUrl- setSearchTerm('simpsons');\n    echo $googleUrl- buildUrl();\n    // https://google.fr/search?q=simpsons  It's also possible to modify it latter      $googleUrl- setHost('google.de');\n    echo $googleUrl- buildUrl();\n    // https://google.de/search?q=simpsons", 
            "title": "Google domain"
        }, 
        {
            "location": "/search-engine/google/google-url/#add-and-remove-parameters", 
            "text": "It's possible to add or remove parameters      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setParam('q', 'simpsons');\n    $googleUrl- setParam('start', 11);\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons start=11\n\n    $googleUrl- removeParam('start');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=simpsons", 
            "title": "Add and remove parameters"
        }, 
        {
            "location": "/search-engine/google/google-url/#raw-parameters", 
            "text": "By default parameters are encoded for urls. For instance  \"Homer Simpsons\"  will become  \"Homer+Simpsons\" \nbut  \"Homer+Simpsons\"  will become  \"Homer%2BSimpson\"      use Serps\\SearchEngine\\Google\\GoogleUrl;\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setParam('q', 'Homer+Simpson');\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=Homer%2BSimpson  It's possible to deal with raw params, this way the param will be passed to the url with no additional encoding. \nThat is achieved by passing true as the third argument of  setParam .      $googleUrl = new GoogleUrl();\n    $googleUrl- setParam('q', 'Homer+Simpson', true);\n    echo $googleUrl- buildUrl();\n    // https://google.com/search?q=Homer+Simpson", 
            "title": "Raw parameters"
        }, 
        {
            "location": "/search-engine/google/google-url/#more-parameters", 
            "text": "Some parameters are very common and for some of them we created convenient shortcuts. See the list:", 
            "title": "More parameters"
        }, 
        {
            "location": "/search-engine/google/google-url/#setsearchterm", 
            "text": "$url- setSearchTerm($searchTerm)  Sets the keywords to search for. Modifies the value for the  q  parameter.", 
            "title": "setSearchTerm"
        }, 
        {
            "location": "/search-engine/google/google-url/#setpage", 
            "text": "$url- setPage($pageNumber)  Sets the page to parse (starts at 0). Modifies the value for the  start  parameter.   Note  If value is less than 0, then the param  start  will be removed from the url", 
            "title": "setPage"
        }, 
        {
            "location": "/search-engine/google/google-url/#setresultsperpage", 
            "text": "$url- setResultsPerPage($resultsPerPages)  Sets the number of results per pages (between 1 and 100). Modifies the value for the  num  parameter.", 
            "title": "setResultsPerPage"
        }, 
        {
            "location": "/search-engine/google/google-url/#setlanguagerestriction", 
            "text": "$url- setLanguageRestriction($lang)  Sets language of the results. Modifies the value for the  lr  parameter. e.g   \"lang_en\" .    Note  \"lang_\"  will be automatically prepended if it is not present.   That means that  $url- setLanguageRestriction('en') \nand  $url- setLanguageRestriction('lang_en')  do the same thing.", 
            "title": "setLanguageRestriction"
        }, 
        {
            "location": "/search-engine/google/google-url/#setautocorrectionenabled", 
            "text": "$url- setAutoCorrectionEnabled($enabled)  Sets if the auto correction should be enabled ( true  or  false ). Modifies the value for the  nfpr  parameter.", 
            "title": "setAutoCorrectionEnabled"
        }, 
        {
            "location": "/search-engine/google/handle-errors/", 
            "text": "Handle erros from the google client\n\n\n\n\n\n\n\nBack to the \ngeneral google documentation\n.\n\n\n\n\nWorking with google involves to make http request, to get an output from google and to parse this output. \nAll of these steps are not error free, you can encounter a network error, an error from google server, get a captcha\nor even face some updated google serp not supported by your version of the library.\n\n\nErrors are managed with exceptions, that makes them easy to handle.\n\n\nRequest error\n\n\nThere are several reasons for a request error to trigger:\n\n\n\n\nA network error\n\n\nA http error\n\n\nA captcha error\n\n\n\n\nAll these errors can be grouped under the same exception type:\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\RequestErrorException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient-\nquery($googleUrl);\n    }catch(RequestErrorException $e){\n        // Some error with the request\n        $errorInfo = $e-\ngetMessage();\n    }\n\n\n\n\nIt's possible to make the distinction between each of them, see bellow.\n\n\nNetwork errors\n\n\nNetwork errors occur when it was not possible to create a valid http connexion with google. They use to be triggered\nby http client, and they can be different for each http client.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\NetworkErrorException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient-\nquery($googleUrl);\n    }catch(NetworkErrorException $e){\n        // Something wrong happened with network\n        $errorInfo = $e-\ngetMessage();\n    }\n\n\n\n\nHttp errors\n\n\nA http error happens when the http server returned a invalid response (status code 404, 500...).\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\HttpResponseErrorException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient-\nquery($googleUrl);\n    }catch(HttpResponseErrorException $e){\n        // Http response is not valid\n        $errorInfo = $e-\ngetMessage();\n    }\n\n\n\n\nCaptcha errors\n\n\nA captcha error happens when the the server asks for a captcha to continue.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\CaptchaException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient-\nquery($googleUrl);\n    }catch(CaptchaException $e){\n        // Captcha required\n        $errorInfo = $e-\ngetMessage();\n    }\n\n\n\n\nDOM errors\n\n\nOnce you got the data from google, you are ready to parse the DOM, but you are not safe against google DOM updates,\nGoogle might have updated its DOM structure and your version of the library is maybe not updated. That's what DOM \nexceptions are made for.\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\SearchEngine\\Google\\Exception\\InvalidDOMException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    $response = $googleClient-\nquery($googleUrl);\n\n    try{\n        $results = $response-\ngetNaturalResults();\n\n        foreach($results as $result){\n            // parse results\n        }\n    }catch(InvalidDOMException $e){\n        // Something bad happened while parsing\n        // Maybe an update of the library is needed, \n        // the exception message maybe tells more\n        $errorInfo = $e-\ngetMessage();\n    }\n\n\n\n\nMore complete example of error handling\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\RequestErrorException;\n    use Serps\\Exception\\RequestError\\HttpResponseErrorException;\n    use Serps\\Exception\\RequestError\\CaptchaException;\n    use Serps\\SearchEngine\\Google\\Exception\\InvalidDOMException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl-\nsetSearchTerm('simpsons');\n\n    $response = null;\n\n    try{\n        $response = $googleClient-\nquery($googleUrl);\n    }catch(HttpResponseErrorException $e){\n        // Http response is not valid, maybe an error from google or your url\n        $errorInfo = $e-\ngetMessage();\n    }catch(CaptchaException $e){\n        // Captcha needs to be solved\n    }catch(RequestErrorException $e){\n        // Other request error are handled here, maybe something wrong with your network\n        $errorInfo = $e-\ngetMessage();\n    }\n\n    if($response){\n        try{\n            $results = $response-\ngetNaturalResults();\n            foreach($results as $result){\n                // parse results\n            }\n        }catch(InvalidDOMException $e){\n            // Something bad happened while parsing\n            // Maybe an update of the library is needed, \n            // the exception message will tell more about\n            $errorInfo = $e-\ngetMessage();\n        }\n    }", 
            "title": "Handle Google error"
        }, 
        {
            "location": "/search-engine/google/handle-errors/#handle-erros-from-the-google-client", 
            "text": "Back to the  general google documentation .   Working with google involves to make http request, to get an output from google and to parse this output. \nAll of these steps are not error free, you can encounter a network error, an error from google server, get a captcha\nor even face some updated google serp not supported by your version of the library.  Errors are managed with exceptions, that makes them easy to handle.", 
            "title": "Handle erros from the google client"
        }, 
        {
            "location": "/search-engine/google/handle-errors/#request-error", 
            "text": "There are several reasons for a request error to trigger:   A network error  A http error  A captcha error   All these errors can be grouped under the same exception type:      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\RequestErrorException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient- query($googleUrl);\n    }catch(RequestErrorException $e){\n        // Some error with the request\n        $errorInfo = $e- getMessage();\n    }  It's possible to make the distinction between each of them, see bellow.", 
            "title": "Request error"
        }, 
        {
            "location": "/search-engine/google/handle-errors/#network-errors", 
            "text": "Network errors occur when it was not possible to create a valid http connexion with google. They use to be triggered\nby http client, and they can be different for each http client.      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\NetworkErrorException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient- query($googleUrl);\n    }catch(NetworkErrorException $e){\n        // Something wrong happened with network\n        $errorInfo = $e- getMessage();\n    }", 
            "title": "Network errors"
        }, 
        {
            "location": "/search-engine/google/handle-errors/#http-errors", 
            "text": "A http error happens when the http server returned a invalid response (status code 404, 500...).      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\HttpResponseErrorException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient- query($googleUrl);\n    }catch(HttpResponseErrorException $e){\n        // Http response is not valid\n        $errorInfo = $e- getMessage();\n    }", 
            "title": "Http errors"
        }, 
        {
            "location": "/search-engine/google/handle-errors/#captcha-errors", 
            "text": "A captcha error happens when the the server asks for a captcha to continue.      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\CaptchaException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    try{\n        $response = $googleClient- query($googleUrl);\n    }catch(CaptchaException $e){\n        // Captcha required\n        $errorInfo = $e- getMessage();\n    }", 
            "title": "Captcha errors"
        }, 
        {
            "location": "/search-engine/google/handle-errors/#dom-errors", 
            "text": "Once you got the data from google, you are ready to parse the DOM, but you are not safe against google DOM updates,\nGoogle might have updated its DOM structure and your version of the library is maybe not updated. That's what DOM \nexceptions are made for.      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\SearchEngine\\Google\\Exception\\InvalidDOMException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    $response = $googleClient- query($googleUrl);\n\n    try{\n        $results = $response- getNaturalResults();\n\n        foreach($results as $result){\n            // parse results\n        }\n    }catch(InvalidDOMException $e){\n        // Something bad happened while parsing\n        // Maybe an update of the library is needed, \n        // the exception message maybe tells more\n        $errorInfo = $e- getMessage();\n    }", 
            "title": "DOM errors"
        }, 
        {
            "location": "/search-engine/google/handle-errors/#more-complete-example-of-error-handling", 
            "text": "use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Exception\\RequestError\\RequestErrorException;\n    use Serps\\Exception\\RequestError\\HttpResponseErrorException;\n    use Serps\\Exception\\RequestError\\CaptchaException;\n    use Serps\\SearchEngine\\Google\\Exception\\InvalidDOMException;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $googleUrl- setSearchTerm('simpsons');\n\n    $response = null;\n\n    try{\n        $response = $googleClient- query($googleUrl);\n    }catch(HttpResponseErrorException $e){\n        // Http response is not valid, maybe an error from google or your url\n        $errorInfo = $e- getMessage();\n    }catch(CaptchaException $e){\n        // Captcha needs to be solved\n    }catch(RequestErrorException $e){\n        // Other request error are handled here, maybe something wrong with your network\n        $errorInfo = $e- getMessage();\n    }\n\n    if($response){\n        try{\n            $results = $response- getNaturalResults();\n            foreach($results as $result){\n                // parse results\n            }\n        }catch(InvalidDOMException $e){\n            // Something bad happened while parsing\n            // Maybe an update of the library is needed, \n            // the exception message will tell more about\n            $errorInfo = $e- getMessage();\n        }\n    }", 
            "title": "More complete example of error handling"
        }, 
        {
            "location": "/search-engine/google/client-configuration/", 
            "text": "Google Client Configuration\n\n\n\n\n\n\n\n\n\nBack to the \ngeneral google documentation\n.\n\n\n\n\nUser agent\n\n\nSetting an user agent is very important\n. \nBy default it will be the one from the http client, and google would easily detect your script as a bot.\n\n\nTo avoid that it happens configure an user agent:\n\n\n    $googleClient-\nrequest-\nsetUserAgent('user agent string');\n\n\n\n\nYou should \nalways\n set a \nreal\n user agent. Here are a few user agent lists:\n\n\n\n\nfrom Chrome\n\n\nfrom Firefox\n\n\nfrom Opera\n\n\nfrom IE\n\n\n\n\nCookie usage\n\n\n\n\nWarning\n\n\nCookies usage is still at prototype stage and all http engines do not support cookies yet.\n\n\n\n\nThe google client can share cookies across several request, thus the state of the client will evolve and persist across\nmany requests.\n\n\nBy default it is disabled, to enable it, simply do:\n\n\n    $googleClient-\nenableCookies();\n\n\n\n\nAnd to disable it again:\n\n\n    $googleClient-\ndisableCookies();\n\n\n\n\nBy default the cookie jar is empty, but you can pass a custom cookie jar:\n\n\n    $googleClient-\nsetCookieJar($cookieJar);\n\n\n\n\nThis way, it's possible to share the cookie jar with other client:\n\n\n    $cookieJar = $googleClient-\ngetCookieJar();\n    $otherGoogleClient-\nsetCookieJar($cookieJar);\n\n\n\n\nView the dedicated \ncookie documentation\n to learn more about cookies manipulation.\n\n\nProxy usage\n\n\nYou can use a proxy at the request time\n\n\n    use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Core\\Http\\Proxy;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $google-\nsetSearchTerm('simpsons');\n\n    $proxy = new Proxy('1.1.1.1', 8080);\n\n    $response = $googleClient-\nquery($googleUrl, $proxy);\n\n\n\n\nSolve a Captcha\n\n\nSolving captcha is not implemented at the moment\n\n\n\n\nNext step: \nparse a page", 
            "title": "Configure Google Client"
        }, 
        {
            "location": "/search-engine/google/client-configuration/#google-client-configuration", 
            "text": "Back to the  general google documentation .", 
            "title": "Google Client Configuration"
        }, 
        {
            "location": "/search-engine/google/client-configuration/#user-agent", 
            "text": "Setting an user agent is very important . \nBy default it will be the one from the http client, and google would easily detect your script as a bot.  To avoid that it happens configure an user agent:      $googleClient- request- setUserAgent('user agent string');  You should  always  set a  real  user agent. Here are a few user agent lists:   from Chrome  from Firefox  from Opera  from IE", 
            "title": "User agent"
        }, 
        {
            "location": "/search-engine/google/client-configuration/#cookie-usage", 
            "text": "Warning  Cookies usage is still at prototype stage and all http engines do not support cookies yet.   The google client can share cookies across several request, thus the state of the client will evolve and persist across\nmany requests.  By default it is disabled, to enable it, simply do:      $googleClient- enableCookies();  And to disable it again:      $googleClient- disableCookies();  By default the cookie jar is empty, but you can pass a custom cookie jar:      $googleClient- setCookieJar($cookieJar);  This way, it's possible to share the cookie jar with other client:      $cookieJar = $googleClient- getCookieJar();\n    $otherGoogleClient- setCookieJar($cookieJar);  View the dedicated  cookie documentation  to learn more about cookies manipulation.", 
            "title": "Cookie usage"
        }, 
        {
            "location": "/search-engine/google/client-configuration/#proxy-usage", 
            "text": "You can use a proxy at the request time      use Serps\\SearchEngine\\Google\\GoogleClient;\n    use Serps\\HttpClient\\CurlClient;\n    use Serps\\SearchEngine\\Google\\GoogleUrl;\n    use Serps\\Core\\Http\\Proxy;\n\n    $googleClient = new GoogleClient(new CurlClient());\n\n    $googleUrl = new GoogleUrl();\n    $google- setSearchTerm('simpsons');\n\n    $proxy = new Proxy('1.1.1.1', 8080);\n\n    $response = $googleClient- query($googleUrl, $proxy);", 
            "title": "Proxy usage"
        }, 
        {
            "location": "/search-engine/google/client-configuration/#solve-a-captcha", 
            "text": "Solving captcha is not implemented at the moment   Next step:  parse a page", 
            "title": "Solve a Captcha"
        }
    ]
}